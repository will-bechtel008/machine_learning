{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hw5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zxns1DhFV2r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQyi_XB-FmEk",
        "colab_type": "text"
      },
      "source": [
        "# General Concepts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55Ubqc_cFxAq",
        "colab_type": "text"
      },
      "source": [
        "## Artificial Intelligence\n",
        "AI is the theory and development of computer systems able to perform tasks that normally require human intelligence, such as visual perception, speech recognition, decision-making, and translation between languages. It is a branch of computer science dealing with the simulation of intelligent behavior in computers; the capability of a machine to imitate intelligent human behavior.\n",
        "\n",
        "## Machine Learning\n",
        "Machine learning is the application of AI by using statistics to find and recognize patterns in large sets of data. There are 3 common subsets of of machine learning:\n",
        "\n",
        "### Supervised Learning\n",
        "Supervised learning is a type of ML where the model is provided with labeled training data. The program is meant to then process unlabeled data, and predict the label for the new data.\n",
        "\n",
        "### Unsupervised Learning\n",
        "Unsupervised learning, however, is a type of ML where the model does not have labeled training data, and learns as it processes the data. It looks for patterns amoung the data, and make informed decisions that way.\n",
        "\n",
        "### Reinforcement Learning\n",
        "Reinforcement learning does not take in labels like the other two. Instead, there is an agent and an environment. A common example is a video game: the agent is taked with \"do not get the game over screen\". It then learns from its environment to learn and become better at reaching that goal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQ8Q7zfdZf2M",
        "colab_type": "text"
      },
      "source": [
        "## Deep Learning\n",
        "Deep learning is a subset of ML where neural networks receive data through a model with many layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIxz424Ia2TL",
        "colab_type": "text"
      },
      "source": [
        "# Basic Concepts\n",
        "Here are the basic concepts that we learned in this class. They set up the core foundation of what you need to know to perform deep learning.\n",
        "\n",
        "## Linear Regression\n",
        "Linear regression is a type of predicitve analysis. It is a learning alorithm that looks at the inpenedent variable and makes predictions based on the dependent variable. Its objective is to find the equation of a line based on the input values. If the line is multidemisional, the equation is:\n",
        "$ y = b + w_1 x_1 + w_2 x_2 + ... + w_n x_n $\n",
        "\n",
        "## Logistic Regression\n",
        "Logistic regression is an algorithm that produces a value between 0 and 1 using a sigmoid curve that determines a binary choice, typically a win/lose decision. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRy410qlePQB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def logistic_regression(data, labels):\n",
        "    # 200 epochs\n",
        "    epochs = 200\n",
        "    # learn rate of 0.001\n",
        "    lr = 0.001\n",
        "\n",
        "    w = np.random.rand(2)\n",
        "    b = np.zeros(1)\n",
        "\n",
        "    i = 0\n",
        "    while i < epochs:\n",
        "        z = w * data[i][0] + w * data[i][1] + b\n",
        "        #sigmoid function\n",
        "        a = 1 / (1 + np.exp(-z))\n",
        "\n",
        "        # gradient of cross-entropy loss\n",
        "        grad_w = (a - labels[i]) * data[i][0] + (a - labels[i]) * data[i][1]\n",
        "        grad_b = a - labels[i]\n",
        "\n",
        "        w = w - (lr * grad_w)\n",
        "        b = b - (lr * grad_b)\n",
        "    return w, b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oqyf4TywiPEG",
        "colab_type": "text"
      },
      "source": [
        "## Conolutions\n",
        "Convolutions are used in convolutional nerual networks, which are used to progressively extract higher and higher level representations of the image content. It takes the raw pixel data as input and learns how to extract these features. It takes a kernal matrix, and preforms the convolution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5U6KnDii62G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def conv2d(input_mat, kernal_mat):\n",
        "    in_size = input_mat.shape[0]\n",
        "    k_size = kernal_mat.shape[0]\n",
        "\n",
        "    # Check if matrices are valid.\n",
        "    if input_mat.shape[0] != input_mat.shape[1]:\n",
        "        print(\"Error: input matrix is not an n x n matrix.\")\n",
        "        return []\n",
        "    elif kernal_mat.shape[0] != kernal_mat.shape[1]:\n",
        "        print(\"Error: kernal matrix is not an n x n matrix.\")\n",
        "        return []\n",
        "    elif k_size > in_size:\n",
        "        print(\"Error: kernal matrix cannot be larger than input matrix.\")\n",
        "        return []\n",
        "    \n",
        "    n = in_size - k_size + 1\n",
        "    if in_size < 1 or k_size < 1 or n < 1:\n",
        "        print(\"Error: Negative dimensions are not allowed.\");\n",
        "        return []\n",
        "    \n",
        "    # Create output matrix\n",
        "    output_mat = np.zeros((n, n))\n",
        "\n",
        "    # Convolution\n",
        "    for i in range(n):\n",
        "        for j in range(n):\n",
        "            output_mat[i, j] = np.sum(input_mat[i:i+k_size, j:j+k_size] * kernal_mat)\n",
        "\n",
        "    return output_mat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgYZXixNjIw1",
        "colab_type": "text"
      },
      "source": [
        "## Max Pooling\n",
        "Max pooling is an operation where the input's size is specified. This allows for the windows to iterate through the matrix and finds a non-overalpping region for the output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4w3zX5gjfAh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def maxpooling2d(input_mat, s):\n",
        "    in_size = input_mat.shape[0]\n",
        "\n",
        "    # Check if matrices are valid.\n",
        "    if input_mat.shape[0] != input_mat.shape[1]:\n",
        "        print(\"Error: input matrix is not an n x n matrix.\")\n",
        "        return []\n",
        "    \n",
        "    # Create dimensions of output matrix\n",
        "    x = int(input_mat.shape[0] / s)\n",
        "    y = int(input_mat.shape[1] / s)\n",
        "    \n",
        "    # Create output matrix\n",
        "    output_mat = np.zeros((x, y))\n",
        "\n",
        "    # Maxpooling\n",
        "    for i in range(x):\n",
        "        for j in range(y):\n",
        "            in_slice = input_mat[i*s:(i+1)*s, j*s:(j+1)*s]\n",
        "            output_mat[i, j] = np.amax(in_slice)\n",
        "\n",
        "    return output_mat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2xVdDVUgSWt",
        "colab_type": "text"
      },
      "source": [
        "## Gradient Descent\n",
        "Gradient descent calculates the slope of the graph for loss minimization. Taking the gradient decent of a loss functions lets the model get closer and closer to being correct."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvGTEEqmj-Ze",
        "colab_type": "text"
      },
      "source": [
        "# Building a Model\n",
        "Convolutional neural networks, as stated above, is a nerual network with additional layers. These types of networks are extremely useful for the classification of of images. These features range from edge detection all the way to species classification. \n",
        "\n",
        "## Convolution layers\n",
        "These layers take the size of the tiles being extracted, as well as the number of filters to apply. The algorithm starts by taking the dot product and then calculates the extracted tile and filter for each pixel of the picture. \n",
        "\n",
        "## Rectified Linear Unit\n",
        "or RELU, is a linear activation function. This is applied to introduce nonlinearity into the model. This allows for more complex images.\n",
        "\n",
        "## Example of the beginning of CNN\n",
        "The code below shows the beginning stages of a CNN construction. This is using the Xception convolutional base."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjErGAfMn0PT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "%tensorflow_version 2.x\n",
        "from keras.applications import Xception\n",
        "\n",
        "conv_base = Xception(\n",
        "    weights='imagenet', \n",
        "    include_top=False, \n",
        "    input_shape=(150, 150, 3))\n",
        "\n",
        "# convolutional base followed by classifier\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(conv_base)\n",
        "\n",
        "# relu introduces nonlinearity \n",
        "# softmax produces classification probabilities \n",
        "model.add(tf.keras.layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zH8BLAbBoSGr",
        "colab_type": "text"
      },
      "source": [
        "# Comping a Model\n",
        "A model tries to minimize loss with the usage of optimizers and learning rates. \n",
        "\n",
        "## Optimizers \n",
        "Optimizers are functions that assist gradient descent. The change the weights in order to obtain faster and more accurate training results.\n",
        "\n",
        "### Stochastic Gradient Descent\n",
        "Stochastic Gradient Descent allows the model to reach faster, accurate predictions. It does so by taking the derivatives of the loss function and multiplies it by the learning rate to approach the desired values. \n",
        "### Learning Rate\n",
        "A learning rate is the number that is multiplied with the resulting gradient to find the next parameters. The learning rate is the size of the steps taken towards the minimized loss. A learning rate has to be finetuned to a model before it is optimal. Typically a value between 0.0001 and 0.01. The learning rate allows us to fine tune our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kv0cPJdoqqU-",
        "colab_type": "text"
      },
      "source": [
        "# Training a Model\n",
        "## Overfitting\n",
        "Overfitting is a situation where the model becomes very familiar and adept at working with the training data. It gets so familiar that it has trouble processing new data sets. \n",
        "This results in low loss for the training data and high loss for new test data. This happens when a model is overcomplicated and is not generalized. \n",
        "\n",
        "## Underfitting\n",
        "Underfitting is when a model can't recognize the training data which produces low accuracy and high loss. Underfitting is typically caused by the opposite of overfitting: the model isn't complex enough. Extra layers and neurons is a possible solution to this problem.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Zm-zOrytc0d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(trainImages, \n",
        "                      trainLabels, \n",
        "                      epochs=100, \n",
        "                      batch_size=128, \n",
        "                      validation_data=(testImages, testLabels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rVkzjsDtjdF",
        "colab_type": "text"
      },
      "source": [
        "# Finetuning a Pretrained Model\n",
        "\n",
        "Finetuning is the process of taking a model that has already been trained for a task and training it to work with a different data set. This is much easier than building a model from scratch. This can be implemented with the addition and removal of layers into the old model.\n",
        "\n",
        "## Convolutional Bases\n",
        "One way to finetune a model is to load in a pretrained convolutional base. This allows us to take a massive model and use it for a particular case in which we want to study. For example, we could take a model that Google has made about animals, and use it on cat's with hats.\n",
        "\n",
        "## Freezing Layers\n",
        "Freezing layers allows us to exclude certain layers while we train. This can be used if we achieve a nice model accuracy of 97%, and we do not want to accidentally change it.\n",
        "\n"
      ]
    }
  ]
}